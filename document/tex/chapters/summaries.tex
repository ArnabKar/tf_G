% !TEX root = ../../document.tex

\documentclass{subfiles}

\begin{document}

  \chapter{Estructuras de Datos de Resumen}
  \label{chapter:summaries}

    \section{Introducción}
    \label{sec:summaries_intro}

      \paragraph{}
      El gran crecimiento tecnológico que se está llevando a cabo en la actualidad a todos los niveles está propiciando además un aumento exponencial en cuanto a la cantidad de información que se genera. La reducción de costes en cuanto a la instalación de sensores que permiten recoger información de muchos procesos productivos, así como la obtenición de metadatos a partir del uso de internet y las redes sociales por parte de los usuarios hace que el ritmo de crecimiento en cuanto a información generada por unidad de tiempo haya crecido a un gran ritmo.

      \paragraph{}
      Una de las razones que han facilitado dicha tendencia es la disminución de costes de almacenamiento de información a la vez que las capacidades de cómputo necesarias para procesar dicha información han aumentado. Sin embargo, debido al crecimiento exponencial en cuanto al tamaño del conjunto de datos, es necesario investigar nuevas técnicas y estrategias que permitan obtener respuestas satisfactorias basadas en la gran cantidad de información de la que se dispone en un tiempo razonable.

      \paragraph{}
      Tradicionalmente, la investigación en el campo de las \emph{bases de datos} se ha centrado en obtener respuestas exactas a distintas consultas, tratando de hacerlo de la manera más eficiente posible, así como de tratar de reducir el espacio necesario para almacenar la información. \emph{Acharya y otros} proponen en el artículo \emph{Join synopses for approximate query answering} \cite{acharya1999join} el concepto de \emph{Approximate Query Processing}. Dicha idea se expone en la subsección \ref{sec:aproximate_query_processing}.

      \subsection{Approximate Query Processing}
      \label{sec:aproximate_query_processing}

        \paragraph{}
        El \emph{procesamiento aproximado de consultas}, (\emph{Approximate Query Processing} o \textbf{AQP}) se presenta como una estrategia de consulta basada en conceptos y propiedades estadísticas que permiten una gran reducción de la complejidad computacional y espacial necesaria para la resolución de consultas a una base de datos. Por contra, dicha reducción a nivel de complejidad tiene como consecuencia la inserción de un determinado nivel de imprecisión en el resultado a la cual denominaremos tasa de error. Se pretende que dicha tasa de error pueda ser acotada en una desviación máxima determinada por $\epsilon$ y se cumpla con un índice de probabilidad $\delta$. Al igual que en capítulos anteriores, en este caso también se presta especial importancia en la minimización del error relativo lo cual consigue que las soluciones mediante el \emph{procesamiento aproximado de consultas} sean válidas tanto para consultas de tamaño reducido como de gran tamaño.


      \paragraph{}
      Durante el resto del capítulo se describen y analizan distintas estrategias que permiten llevar a cabo implementaciones basadas en \emph{procesamiento aproximado de consultas} centrando especial atención en los \emph{Sketches} por su similitud con el \emph{Modelo en Streaming} descrito en el capítulo \ref{chapter:streaming}. En la sección \ref{sec:summaries_types} se realiza una decripción a partir de la cual se pretende aclarar las diferencias entre las distintas \emph{estructuras de datos de resumen}. Posteriormente, en la sección \ref{sec:sketching} se explican en detalle las cualidades de las estrategias basadas en \emph{Sketching}. En las secciones \ref{sec:count_min_sketch}, \ref{sec:count_sketch}, \ref{sec:ams_sketch} y \ref{sec:hyper_log_log} se habla de \emph{Count-Min Sketch}, \emph{Count Sketch}, \emph{AMS Sketch} e \emph{HyperLogLog} respectivamente.


    \section{Tipos de Estructuras de Datos de Resumen}
    \label{sec:summaries_types}

      \paragraph{}
      Para el diseño de soluciones basadas en \emph{procesamiento aproximado de consultas} en bases de datos existen distintas estrategias, las cuales presentan distintas ventajas e inconvenientes tal y como se pretende mostrar en esta sección. Dichas descripciones han sido extraidas del libro \emph{Synopses for massive data} \cite{cormode2012synopses} redactado por \emph{Cormode y otros}. En las secciones \ref{sec:sampling}, \ref{sec:histogram}, \ref{sec:wavelet} y \ref{sec:sketch} se habla de \emph{Sampling}, \emph{Histogram}, \emph{Wavelet} y \emph{Sketches} respectivamente.

      \subsection{Sampling}
      \label{sec:sampling}

        \paragraph{}
        El \emph{Sampling} o \emph{muestreo} es la estrategia más consolidada entre las que se presentan. Las razones se deben a su simplicidad conceptual así como su extendido uso en el mundo de la estadística. Uno de los primeros artículos en que se trata el muestreo aplicado a bases de datos es \emph{Accurate estimation of the number of tuples satisfying a condition} \cite{piatetsky1984accurate} redactado por \emph{Piatetsky-Shapiro} y \emph{Connell}. La intuición en que se basa dicha estrategia es la selección de un subconjunto de elementos denominado \emph{muestra} de entre el conjunto global al cual se denomina \emph{población}. Una vez obtenida la \emph{muestra} del conjunto de datos global cuyo tamaño es significativamente menor (lo cual reduce drásticamente el coste computacional), se realizan los cálculos que se pretendía realizar sobre toda la \emph{población}, a partir de los cuales se obtiene un estimador del valor real que habría sido obtenido al realizarlos sobre el conjunto de datos global.

        \paragraph{}
        Para que las estrategias de sumarización de información obtengan resultados válidos o significativos respecto del conjunto de datos, es necesario que se escojan adecuadamente las instancias de la \emph{muestra}, de manera que represente de manera fiel la información global. Para llevar a cabo dicha labor existen distintas estrategias, desde las más simples basadas en la selección aleatoria sin reemplazamiento como otras mucho más sofisticadas basadas en el mantenimiento de \emph{muestras} estratificadas. Sea $R$ la población y $|R|$ el tamaño de la misma. Denominaremos $t_j$ al valor $j$-ésimo de la población y $X_j$ al número de ocurrencias del mismo en la \emph{muestra}. A continuación se describen distintas técnicas de muestreo:

        \begin{itemize}

          \item \textbf{Selección Aleatoria Sin Reemplazamiento}: Consiste en la estrategia más simple de generación de \emph{muestras}. Se basa en la selección aleatoria de un valor entero $r$ en el rango $[1, |R|]$ para después añadir el elemento localizado en la posición $r$ de la \emph{población} al subconjunto de \emph{muestra}. Después repetir dicha secuencia durante $n$ veces para generar una \emph{muestra} de tamaño $n$. El estimador para la operación \emph{SUMA} se muestra en la ecuación \eqref{eq:sum_with_replacement} además de la desviación de dicho estimador en la ecuación \eqref{eq:sum_with_replacement_deviation}.
            \begin{align}
            \label{eq:sum_with_replacement}
              Y &= \frac{|R|}{n}\sum_jX_jt_j \\
            \label{eq:sum_with_replacement_deviation}
              \sigma^2(Y) &= \frac{|R|^2\sigma^2(R)}{n}
            \end{align}

          \item \textbf{Selección Aleatoria Con Reemplazamiento}: En este caso se supone que la selección de una instancia de la población tan solo se puede llevar a cabo una única vez como mucho, por lo tanto se cumple que $\forall X_j \in {0,1}$. La selección se lleva a cabo de la siguiente manera: se genera de manera aleatoria un valor entero $r$ en el rango $[1, |R|]$ para después añadir el elemento localizado en la posición $r$ de la \emph{población} al subconjunto de \emph{muestra} si este no ha sido añadido ya, sino volver a generar otro valor $r$. Después repetir dicha secuencia durante $n$ veces para generar una \emph{muestra} de tamaño $n$. Al igual que en la estrategia anterior, en este caso también se muestra el estimador para la operación \emph{SUMA} en la ecuación \eqref{eq:sum_without_replacement}. Nótese que el cálculo es el mismo que en el caso de la estrategia sin reemplazamiento. Sin embargo, la varianza obtenida a partir de dicha estrategia es menor tal y como se muestra en la ecuación \eqref{eq:sum_without_replacement_deviation}.
            \begin{align}
            \label{eq:sum_without_replacement}
              Y &= \frac{|R|}{n}\sum_jX_jt_j \\
            \label{eq:sum_without_replacement_deviation}
              \sigma^2(Y) &= \frac{|R|(|R| - n)\sigma^2(R)}{n}
            \end{align}

          \item \textbf{Bernoulli y Poisson}: Mediante esta alternativa de muestreo se sigue una estrategia completamente distinta a las anteriores. En lugar de seleccionar la siguiente instancia aleatoriamente de entre todas las posibles, se decide generar $|R|$ valores aleatorios $r_j$ independientes en el intervalo $[0,1]$ de tal manera que si $r_j$ es menor que un valor $p_j$ fijado a priori, la instancia se añade al conjunto de \emph{muestra}. Cuando se cumple que $\forall i, j \ p_i = p_j$ se dice que es un muestreo de \emph{Bernoulli}, mientras que cuando no se cumple dicha condición se habla de muestreo de \emph{Poisson}. El cálculo de la \emph{SUMA} en este caso es muy diferente de los anteriores tal y como se muestra en la ecuación \eqref{eq:sum_bernoulli_poisson}. La desviación de este estimador se muestra en la ecuación \eqref{eq:sum_bernoulli_poisson_deviation}, que en general presenta peores resultados (mayor desviación) que las anteriores alternativas, sin embargo, esta alternativa posee la cualidad de aplicar distintos pesos a cada instancia de la población, lo que puede traducirse en que una selección adecuada de dichos valores $p_j$ puede mejorar significativamente dichos resultados.
            \begin{align}
            \label{eq:sum_bernoulli_poisson}
              Y &= \sum_{i \in muestra }\frac{t_i}{p_i} \\
            \label{eq:sum_bernoulli_poisson_deviation}
              \sigma^2(Y) &= \sum_i(\frac{1}{p_i}-1)t_i^2
            \end{align}

          \item \textbf{Muestreo Estratificado}: El muestreo estratificado trata de minimizar al máximo las diferencias entre la distribución del conjunto de datos de la \emph{población} de la \emph{muestra} que se pretende generar. Para ello existen distintas alternativas entre las que se encuentra una selección que  actualiza los pesos $p_j$ tras cada iteracción, lo que reduce la desviación de la \emph{muestra}, sin embargo produce un elevado coste computacional en su generación. Por lo tanto se proponen otras estrategia más intuitiva basada en la partición del conjunto de datos de la \emph{población} en subconjuntos disjuntos con varianza mínima entre las instancias que contienen a los cuales se denomina \emph{estratos}. Posteriormente se selecciona mediante cualquiera de los métodos anteriores una \emph{muestra} de cada \emph{estrato}, lo cual reduce en gran medida la desviación típica global del estimador.

        \end{itemize}

        \paragraph{}
        La estrategia de sumarización de información mediante \emph{muestreo} tiene como ventajas la independencia de la complejidad con respecto a la dimensionalidad de los datos (algo que como se verá a continuación no sucede con otras alternativas) además de su simplicidad conceptual. También existen cotas de error para las consultas, para las cuales no ofrece restricciones en cuanto al tipo (debido a que se realizan sobre un subconjunto con la misma estructura que el global). El muestre es apropiado para conocer información general acerca del conjunto de datos que cada instancia del mismo posee. Además, presenta la cualidad de permitir la modificación en tiempo real, es decir, se pueden añadir o eliminar nuevas instancias a la muestra conforme se añaden o eliminan del conjunto de datos global.

        \paragraph{}
        Sin embargo, en entornos donde el ratio de addiciones/eliminaciones es muy elevado el coste del mantenimiento de la muestra puede hacerse impracticable . El \emph{muestreo} es una buena alternativa para conjuntos de datos homogéneos en los cuales la presencia de valores atípicos es irrelevante. Tampoco obtiene buenos resultados en consultas relacionadas con el conteo de elementos distintos. En las siguientes secciones se describen alternativas que resuelven estas dificultades y limitaciones.

      \subsection{Histogram}
      \label{sec:histogram}

        \paragraph{}
        Los \emph{histogramas} son estructuras de datos utilizadas para sumarizar grandes conjuntos de datos, pero por contra, tienen un enfoque completamente diferente al que siguen las estrategias de \emph{muestreo} de la sección anterior. En este caso, el concepto es similar a la visión estadística de los histogramas. Consiste en dividir el dominio de valores que pueden tomar las instancias del conjunto de datos de tal manera que se mantiene un conteo del número de instancias pertenecientes a cada partición.

        \paragraph{}
        Durante el resto de la sección se describen de manera resumida distintas estrategias de estimación del valor de las particiones así como las distintas estrategias de particionamiento del conjunto de datos. Para llevar a cabo dicha labor es necesario describir la notación que se seguirá: Sea $D$ el conjunto de datos e $i \in [1,M]$ cada una de las categorías de los mismos. Denotaremos por $g(i)$ el número de ocurrencias del valor $i$. Para referirnos a cada uno de las particiones utilizaremos la notación $S_j$ para $j \in [1, B]$. Nótese por tanto que $M$ representa el número de categorías distintas mientras que $B$ es el número de particiones utilizadas para \say{comprimir} los datos. La mejora de eficiencia en cuanto a espacio se consigue devido a la presuposición de que $B \ll M$


        \paragraph{}
        Cuando se habla de \emph{esquemas de estimación} se trata de describir la manera en que se almacena o trata el contenido de cada una de las particiones $S_j$ del histograma. La razón por la cual este es un factor importante a la hora de caracterizar un histograma es debida a que está altamente ligada a la precisión del mismo.

        \begin{itemize}

          \item \textbf{Esquema Uniforme}: Los esquemas que presuponen distribución uniforme se subdividen en dos categorías: \begin{enumerate*} [label=\itshape\alph*\upshape)]
        			\item \emph{continous-value asumption} que presupone que todas las categorías $i$ contenidas en la partición $S_j$ presentan el mismo valor para la función $g(i)$ y
        			\item \emph{uniform-spread asumption} que presupone que el número de ocurrencias de la partición $S_j$ se localiza distribuido uniformemente al igual que en el caso anterior, pero en este caso entre los elementos de un subconjunto $P_j$ generado iterando con un determinado desplazamiento $k$ sobre las categorías $i$ contenidas en $S_j$.
        		\end{enumerate*} El segundo enfoque presenta mejores resultados en el caso de consultas de cuantiles que se distribuyen sobre más de una partición $S_j$

          \item \textbf{Esquema Basado en Splines}: En la estrategia basada en splines se presupone que los valores se distribuyen conforme una determinada función lineal de la forma $y_j = a_jx_j + b_j$ en cada partición $S_j$ de tal manera que el conjunto total de datos $D$ puede verse como una función continua a trozos. Nótese que en este caso se habla de una función lineal, sin embargo puede generalizarse a funciones no lineales.

          \item \textbf{Esquema Basado en Árboles}: Consiste en el almacenamiento de las frecuencias de cada partición $S_j$ en forma de árbol binario, lo cual permite seleccionar de manera apropiada el nivel del árbol que reduzca el número de operaciones necesarias para obtener la estimación del número de ocurrencias según el tamaño rango de la consulta. La razón por la cual se escoje un árbol binario es debida a que se puede reducir en un orden de $2$ el espacio necesario para almacenar dichos valores manteniendo únicamente los de una de las ramas del mismo. La razón de ello es debida a que se puede calcular el valor de la otra mediante una resta sobre el valor almacenado en el nodo padre y la rama que si contiene el valor.

          \item \textbf{Esquema Heterogéneo}: El esquema heterogéneo se basa la intuición de que la distribución de frecuencias de cada una de las particiones $S_j$ no es uniforme, por lo tanto sigue un enfoque diferente en cada una de ellas tratanto de minimizar al máximo la tasa de error producida. Para ello existen  distintas heurísticas basadas en distancias o teoría de la información entre otros.

        \end{itemize}


        \paragraph{}
        Una vez descritas distintas estrategias de estimación del valor de frecuencias de una determinada partición $S_j$, el siguiente paso para describir un \emph{histograma} es realizar una descripción acerca de las distintas formas de generación de dichas particiones. Para tratar de ajustarse de manera más adecuada a la distribución de los datos, se realiza un \emph{muestreo} con el cual se generan las particiones. A continuación se describen las técnicas más comunes para dicha labor:

        \begin{itemize}

          \item \textbf{Particionamiento Heurístico}:[TODO ]

          \item \textbf{Particionamiento con Garantías de Optimalidad}:[TODO ]

          \item \textbf{Particionamiento Jerárquico}:[TODO ]

        \end{itemize}

        \paragraph{}
        [TODO Hablar de multidimensionalidad]

        \paragraph{}
        [TODO Ventajas y desventajas]

      \subsection{Wavelet}
      \label{sec:wavelet}

        \paragraph{}
        [TODO ]

      \subsection{Sketch}
      \label{sec:sketch}

        \paragraph{}
        [TODO ]

    \section{Sketching}
    \label{sec:sketching}

      \paragraph{}
      [TODO ]

    \section{Count-Min Sketch}
    \label{sec:count_min_sketch}

      \paragraph{}
      [TODO ] \emph{An improved data stream summary: the count-min sketch and its applications} \cite{cormode2005improved}

    \section{Count Sketch}
    \label{sec:count_sketch}

      \paragraph{}
      [TODO ] \emph{Finding frequent items in data streams} \cite{charikar2002finding}

    \section{AMS Sketch}
    \label{sec:ams_sketch}

      \paragraph{}
      [TODO ] \emph{The space complexity of approximating the frequency moments} \cite{alon1996space}

    \section{HyperLogLog}
    \label{sec:hyper_log_log}

      \paragraph{}
      [TODO ] \emph{Hyperloglog: the analysis of a near-optimal cardinality estimation algorithm} \cite{flajolet2007hyperloglog}


\end{document}
