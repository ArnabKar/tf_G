% !TEX root = ../../document.tex

\documentclass{subfiles}

\begin{document}

  \chapter{Estructuras de Datos de Resumen}
  \label{chapter:summaries}

    \section{Introducción}
    \label{sec:summaries_intro}

      \paragraph{}
      El gran crecimiento tecnológico que se está llevando a cabo en la actualidad a todos los niveles está propiciando además un aumento exponencial en cuanto a la cantidad de información que se genera. La reducción de costes en cuanto a la instalación de sensores que permiten recoger información de muchos procesos productivos, así como la obtenición de metadatos a partir del uso de internet y las redes sociales por parte de los usuarios hace que el ritmo de crecimiento en cuanto a información generada por unidad de tiempo haya crecido a un gran ritmo.

      \paragraph{}
      Una de las razones que han facilitado dicha tendencia es la disminución de costes de almacenamiento de información a la vez que las capacidades de cómputo necesarias para procesar dicha información han aumentado. Sin embargo, debido al crecimiento exponencial en cuanto al tamaño del conjunto de datos, es necesario investigar nuevas técnicas y estrategias que permitan obtener respuestas satisfactorias basadas en la gran cantidad de información de la que se dispone en un tiempo razonable.

      \paragraph{}
      Tradicionalmente, la investigación en el campo de las \emph{bases de datos} se ha centrado en obtener respuestas exactas a distintas consultas, tratando de hacerlo de la manera más eficiente posible, así como de tratar de reducir el espacio necesario para almacenar la información. \emph{Acharya y otros} proponen en el artículo \emph{Join synopses for approximate query answering} \cite{acharya1999join} el concepto de \emph{Approximate Query Processing}. Dicha idea se expone en la subsección \ref{sec:aproximate_query_processing}.

      \subsection{Approximate Query Processing}
      \label{sec:aproximate_query_processing}

        \paragraph{}
        El \emph{procesamiento aproximado de consultas}, (\emph{Approximate Query Processing} o \textbf{AQP}) se presenta como una estrategia de consulta basada en conceptos y propiedades estadísticas que permiten una gran reducción de la complejidad computacional y espacial necesaria para la resolución de consultas a una base de datos. Por contra, dicha reducción a nivel de complejidad tiene como consecuencia la inserción de un determinado nivel de imprecisión en el resultado a la cual denominaremos tasa de error. Se pretende que dicha tasa de error pueda ser acotada en una desviación máxima determinada por $\epsilon$ y se cumpla con un índice de probabilidad $\delta$. Al igual que en capítulos anteriores, en este caso también se presta especial importancia en la minimización del error relativo lo cual consigue que las soluciones mediante el \emph{procesamiento aproximado de consultas} sean válidas tanto para consultas de tamaño reducido como de gran tamaño.


      \paragraph{}
      Durante el resto del capítulo se describen y analizan distintas estrategias que permiten llevar a cabo implementaciones basadas en \emph{procesamiento aproximado de consultas} centrando especial atención en los \emph{Sketches} por su similitud con el \emph{Modelo en Streaming} descrito en el capítulo \ref{chapter:streaming}. En la sección \ref{sec:summaries_types} se realiza una decripción a partir de la cual se pretende aclarar las diferencias entre las distintas \emph{estructuras de datos de resumen}. Posteriormente, en la sección \ref{sec:sketching} se explican en detalle las cualidades de las estrategias basadas en \emph{Sketching}. En las secciones \ref{sec:count_min_sketch}, \ref{sec:count_sketch}, \ref{sec:ams_sketch} y \ref{sec:hyper_log_log} se habla de \emph{Count-Min Sketch}, \emph{Count Sketch}, \emph{AMS Sketch} e \emph{HyperLogLog} respectivamente.


    \section{Tipos de Estructuras de Datos de Resumen}
    \label{sec:summaries_types}

      \paragraph{}
      Para el diseño de soluciones basadas en \emph{procesamiento aproximado de consultas} en bases de datos existen distintas estrategias, las cuales presentan distintas ventajas e inconvenientes tal y como se pretende mostrar en esta sección. Dichas descripciones han sido extraidas del libro \emph{Synopses for massive data} \cite{cormode2012synopses} redactado por \emph{Cormode y otros}. En las secciones \ref{sec:sampling}, \ref{sec:histogram}, \ref{sec:wavelet} y \ref{sec:sketch} se habla de \emph{Sampling}, \emph{Histogram}, \emph{Wavelet} y \emph{Sketches} respectivamente.

      \subsection{Sampling}
      \label{sec:sampling}

        \paragraph{}
        El \emph{Sampling} o \emph{muestreo} es la estrategia más consolidada entre las que se presentan. Las razones se deben a su simplicidad conceptual así como su extendido uso en el mundo de la estadística. Uno de los primeros artículos en que se trata el muestreo aplicado a bases de datos es \emph{Accurate estimation of the number of tuples satisfying a condition} \cite{piatetsky1984accurate} redactado por \emph{Piatetsky-Shapiro} y \emph{Connell}. La intuición en que se basa dicha estrategia es la selección de un subconjunto de elementos denominado \emph{muestra} de entre el conjunto global al cual se denomina \emph{población}. Una vez obtenida la \emph{muestra} del conjunto de datos global cuyo tamaño es significativamente menor (lo cual reduce drásticamente el coste computacional), se realizan los cálculos que se pretendía realizar sobre toda la \emph{población}, a partir de los cuales se obtiene un estimador del valor real que habría sido obtenido al realizarlos sobre el conjunto de datos global.

        \paragraph{}
        Para que las estrategias de sumarización de información obtengan resultados válidos o significativos respecto del conjunto de datos, es necesario que se escojan adecuadamente las instancias de la \emph{muestra}, de manera que represente de manera fiel la información global. Para llevar a cabo dicha labor existen distintas estrategias, desde las más simples basadas en la selección aleatoria sin reemplazamiento como otras mucho más sofisticadas basadas en el mantenimiento de \emph{muestras} estratificadas. Sea $R$ la población y $|R|$ el tamaño de la misma. Denominaremos $t_j$ al valor $j$-ésimo de la población y $X_j$ al número de ocurrencias del mismo en la \emph{muestra}. A continuación se describen distintas técnicas de muestreo:

        \begin{itemize}

          \item \textbf{Selección Aleatoria Sin Reemplazamiento}: Consiste en la estrategia más simple de generación de \emph{muestras}. Se basa en la selección aleatoria de un valor entero $r$ en el rango $[1, |R|]$ para después añadir el elemento localizado en la posición $r$ de la \emph{población} al subconjunto de \emph{muestra}. Después repetir dicha secuencia durante $n$ veces para generar una \emph{muestra} de tamaño $n$. El estimador para la operación \emph{SUMA} se muestra en la ecuación \eqref{eq:sum_with_replacement} además de la desviación de dicho estimador en la ecuación \eqref{eq:sum_with_replacement_deviation}.
            \begin{align}
            \label{eq:sum_with_replacement}
              Y &= \frac{|R|}{n}\sum_jX_jt_j \\
            \label{eq:sum_with_replacement_deviation}
              \sigma^2(Y) &= \frac{|R|^2\sigma^2(R)}{n}
            \end{align}

          \item \textbf{Selección Aleatoria Con Reemplazamiento}: En este caso se supone que la selección de una instancia de la población tan solo se puede llevar a cabo una única vez como mucho, por lo tanto se cumple que $\forall X_j \in {0,1}$. La selección se lleva a cabo de la siguiente manera: se genera de manera aleatoria un valor entero $r$ en el rango $[1, |R|]$ para después añadir el elemento localizado en la posición $r$ de la \emph{población} al subconjunto de \emph{muestra} si este no ha sido añadido ya, sino volver a generar otro valor $r$. Después repetir dicha secuencia durante $n$ veces para generar una \emph{muestra} de tamaño $n$. Al igual que en la estrategia anterior, en este caso también se muestra el estimador para la operación \emph{SUMA} en la ecuación \eqref{eq:sum_without_replacement}. Nótese que el cálculo es el mismo que en el caso de la estrategia sin reemplazamiento. Sin embargo, la varianza obtenida a partir de dicha estrategia es menor tal y como se muestra en la ecuación \eqref{eq:sum_without_replacement_deviation}.
            \begin{align}
            \label{eq:sum_without_replacement}
              Y &= \frac{|R|}{n}\sum_jX_jt_j \\
            \label{eq:sum_without_replacement_deviation}
              \sigma^2(Y) &= \frac{|R|(|R| - n)\sigma^2(R)}{n}
            \end{align}

          \item \textbf{Bernoulli y Poisson}: Mediante esta alternativa de muestreo se sigue una estrategia completamente distinta a las anteriores. En lugar de seleccionar la siguiente instancia aleatoriamente de entre todas las posibles, se decide generar $|R|$ valores aleatorios $r_j$ independientes en el intervalo $[0,1]$ de tal manera que si $r_j$ es menor que un valor $p_j$ fijado a priori, la instancia se añade al conjunto de \emph{muestra}. Cuando se cumple que $\forall i, j \ p_i = p_j$ se dice que es un muestreo de \emph{Bernoulli}, mientras que cuando no se cumple dicha condición se habla de muestreo de \emph{Poisson}. El cálculo de la \emph{SUMA} en este caso es muy diferente de los anteriores tal y como se muestra en la ecuación \eqref{eq:sum_bernoulli_poisson}. La desviación de este estimador se muestra en la ecuación \eqref{eq:sum_bernoulli_poisson_deviation}, que en general presenta peores resultados (mayor desviación) que las anteriores alternativas, sin embargo, esta alternativa posee la cualidad de aplicar distintos pesos a cada instancia de la población, lo que puede traducirse en que una selección adecuada de dichos valores $p_j$ puede mejorar significativamente dichos resultados.
            \begin{align}
            \label{eq:sum_bernoulli_poisson}
              Y &= \sum_{i \in muestra }\frac{t_i}{p_i} \\
            \label{eq:sum_bernoulli_poisson_deviation}
              \sigma^2(Y) &= \sum_i(\frac{1}{p_i}-1)t_i^2
            \end{align}

          \item \textbf{Muestreo Estratificado}: El muestreo estratificado trata de minimizar al máximo las diferencias entre la distribución del conjunto de datos de la \emph{población} de la \emph{muestra} que se pretende generar. Para ello existen distintas alternativas entre las que se encuentra una selección que  actualiza los pesos $p_j$ tras cada iteracción, lo que reduce la desviación de la \emph{muestra}, sin embargo produce un elevado coste computacional en su generación. Por lo tanto se proponen otras estrategia más intuitiva basada en la partición del conjunto de datos de la \emph{población} en subconjuntos disjuntos con varianza mínima entre las instancias que contienen a los cuales se denomina \emph{estratos}. Posteriormente se selecciona mediante cualquiera de los métodos anteriores una \emph{muestra} de cada \emph{estrato}, lo cual reduce en gran medida la desviación típica global del estimador.

        \end{itemize}

        \paragraph{}
        La estrategia de sumarización de información mediante \emph{muestreo} tiene como ventajas la independencia de la complejidad con respecto a la dimensionalidad de los datos (algo que como se verá a continuación no sucede con otras alternativas) además de su simplicidad conceptual. También existen cotas de error para las consultas, para las cuales no ofrece restricciones en cuanto al tipo (debido a que se realizan sobre un subconjunto con la misma estructura que el global). El muestre es apropiado para conocer información general acerca del conjunto de datos que cada instancia del mismo posee. Además, presenta la cualidad de permitir la modificación en tiempo real, es decir, se pueden añadir o eliminar nuevas instancias a la muestra conforme se añaden o eliminan del conjunto de datos global.

        \paragraph{}
        Sin embargo, en entornos donde el ratio de addiciones/eliminaciones es muy elevado el coste del mantenimiento de la muestra puede hacerse impracticable . El \emph{muestreo} es una buena alternativa para conjuntos de datos homogéneos en los cuales la presencia de valores atípicos es irrelevante. Tampoco obtiene buenos resultados en consultas relacionadas con el conteo de elementos distintos. En las siguientes secciones se describen alternativas que resuelven estas dificultades y limitaciones.

      \subsection{Histogram}
      \label{sec:histogram}

        \paragraph{}
        Los \emph{histogramas} son estructuras de datos utilizadas para sumarizar grandes conjuntos de datos, pero por contra, tienen un enfoque completamente diferente al que siguen las estrategias de \emph{muestreo} de la sección anterior. En este caso, el concepto es similar a la visión estadística de los histogramas. Consiste en dividir el dominio de valores que pueden tomar las instancias del conjunto de datos de tal manera que se mantiene un conteo del número de instancias pertenecientes a cada partición.

        \paragraph{}
        Durante el resto de la sección se describen de manera resumida distintas estrategias de estimación del valor de las particiones así como las distintas estrategias de particionamiento del conjunto de datos. Para llevar a cabo dicha labor es necesario describir la notación que se seguirá: Sea $D$ el conjunto de datos e $i \in [1,M]$ cada una de las categorías de los mismos. Denotaremos por $g(i)$ el número de ocurrencias del valor $i$. Para referirnos a cada uno de las particiones utilizaremos la notación $S_j$ para $j \in [1, B]$. Nótese por tanto que $M$ representa el número de categorías distintas mientras que $B$ es el número de particiones utilizadas para \say{comprimir} los datos. La mejora de eficiencia en cuanto a espacio se consigue devido a la presuposición de que $B \ll M$


        \paragraph{}
        Cuando se habla de \emph{esquemas de estimación} se trata de describir la manera en que se almacena o trata el contenido de cada una de las particiones $S_j$ del histograma. La razón por la cual este es un factor importante a la hora de caracterizar un histograma es debida a que está altamente ligada a la precisión del mismo.

        \begin{itemize}

          \item \textbf{Esquema Uniforme}: Los esquemas que presuponen distribución uniforme se subdividen en dos categorías: \begin{enumerate*} [label=\itshape\alph*\upshape)]
        			\item \emph{continous-value asumption} que presupone que todas las categorías $i$ contenidas en la partición $S_j$ presentan el mismo valor para la función $g(i)$ y
        			\item \emph{uniform-spread asumption} que presupone que el número de ocurrencias de la partición $S_j$ se localiza distribuido uniformemente al igual que en el caso anterior, pero en este caso entre los elementos de un subconjunto $P_j$ generado iterando con un determinado desplazamiento $k$ sobre las categorías $i$ contenidas en $S_j$.
        		\end{enumerate*} El segundo enfoque presenta mejores resultados en el caso de consultas de cuantiles que se distribuyen sobre más de una partición $S_j$

          \item \textbf{Esquema Basado en Splines}: En la estrategia basada en splines se presupone que los valores se distribuyen conforme una determinada función lineal de la forma $y_j = a_jx_j + b_j$ en cada partición $S_j$ de tal manera que el conjunto total de datos $D$ puede verse como una función continua a trozos. Nótese que en este caso se habla de una función lineal, sin embargo puede generalizarse a funciones no lineales.

          \item \textbf{Esquema Basado en Árboles}: Consiste en el almacenamiento de las frecuencias de cada partición $S_j$ en forma de árbol binario, lo cual permite seleccionar de manera apropiada el nivel del árbol que reduzca el número de operaciones necesarias para obtener la estimación del número de ocurrencias según el tamaño rango de la consulta. La razón por la cual se escoje un árbol binario es debida a que se puede reducir en un orden de $2$ el espacio necesario para almacenar dichos valores manteniendo únicamente los de una de las ramas del mismo. La razón de ello es debida a que se puede calcular el valor de la otra mediante una resta sobre el valor almacenado en el nodo padre y la rama que si contiene el valor.

          \item \textbf{Esquema Heterogéneo}: El esquema heterogéneo se basa la intuición de que la distribución de frecuencias de cada una de las particiones $S_j$ no es uniforme, por lo tanto sigue un enfoque diferente en cada una de ellas tratanto de minimizar al máximo la tasa de error producida. Para ello existen  distintas heurísticas basadas en distancias o teoría de la información entre otros.

        \end{itemize}


        \paragraph{}
        Una vez descritas distintas estrategias de estimación del valor de frecuencias de una determinada partición $S_j$, el siguiente paso para describir un \emph{histograma} es realizar una descripción acerca de las distintas formas de generación de dichas particiones. Para tratar de ajustarse de manera más adecuada a la distribución de los datos, se realiza un \emph{muestreo} con el cual se generan las particiones. A continuación se describen las técnicas más comunes para dicha labor:

        \begin{itemize}

          \item \textbf{Particionamiento Heurístico}: Dichas estrategias de particionamiento se basan en distintas heurísticas que en la práctica han demostrado comportamientos aceptables en cuanto a resultados a nivel de precisión, sin embargo, no proporcionan ninguna garantía a nivel de optimalidad. Su uso está ampliamente extendido debido al reducido coste computacional. Dentro de esta categoría las heurísticas más populares son las siguientes:
            \begin{itemize}

              \item \textbf{Equi-Width}: Consiste en la división del dominio de categorías $[1,M]$ en particiones equi-espaciadas unas de otras. Para dicha estrategia tan solo es necesario conocer \emph{a-priori} el rango del posible conjunto de valores. Es la solución con menor coste computacional, a pesar de ello sus resultados a nivel práctico son similares a otras estrategias más sofisticadas cuando la distribución de frecuencias es uniforme.

              \item \textbf{Equi-Depth}: Esta estrategia de particionamiento requiere conocer la distribución de frecuencias \emph{a-priori} (o aproximarla a partir de algún método de muestreo). Se basa en la división del dominio de valores de tal manera que las particiones tengan la misma frecuencia. Para ello se crean particiones de tamaños diferentes.

              \item \textbf{Singleton-Bucket}: Para tratar de mejorar la precisión esta estrategia de particionamiento se basa en la utilización de dos particiones especiales, las cuales contienen las categorías de mayor y menor frecuencia respectivamente para después cubrir el resto de categorías restante mediante otra estrategia (generalmente \emph{equi-depth}).

              \item \textbf{Maxdiff}: En este caso, el método de particionamiento se apoya en la idea de utilizar los puntos de mayor variación de frecuencias mediante la medida $|g(i+1) - g(i)|$, para dividir el conjunto de categorías en sus respectivas particiones, de tal manera que las frecuencias contenidas en cada partición sean lo más homogéneas posibles.

            \end{itemize}
          \item \textbf{Particionamiento con Garantías de Optimalidad}: En esta categoría se enmarcan las estrategias de generación de particiones que ofrecen garantías de optimalidad a nivel de la precisión de resultados en las consultas. Para ello se apoyan en técnicas de \emph{Programación Dinámica} (DP), de tal manera que la selección de las particiones se presenta como un problema de \emph{Optimización}. Sin embargo, dichas estrategias presentan un elevado coste computacional que muchas veces no es admisible debido al gran tamaño del conjunto de datos que se pretende sumarizar. Como solución ante dicha problemática se han propuesto distintas estrategias que se basan en la resolución del problema de optimización, pero sobre una \emph{muestra} del conjunto de datos global, lo cual anula las garantías de optimalidad pero si se escoge de manera adecuada ofrece una buena aproximación hacia ellas.

          \item \textbf{Particionamiento Jerárquico}: Las estrategias de particionamiento jerárquico se basan en la utilización de particiones siguiendo la idea de un árbol binario. Por lo tanto, dichas particiones no son disjuntas entre ellas, sino que se contienen unas a otras. Esto sigue la misma idea que se describió en el apartado de \emph{Esquemas de estimación Basados en Árboles}. Apoyandose en este estilo de particionamiento se consigue que las consultas de rangos de frecuencias tengan un coste computacional menor en promedio (aún en el casos en que el rango sea muy amplio). En esta categoría destacan los histogramas \emph{nLT} (n-level Tree) y \emph{Lattice Histograms}. Estos últimos tratan de aprovechar las ventajas a nivel de flexibilidad y precisión que presentan los histogramas, además de las estrategias jerárquicas de sumarización en que se apoyan las \emph{Wavelets} tal y como se describe en la siguiente sección.

        \end{itemize}

        \paragraph{}
        Las ideas descritas en esta sección sobre los \emph{histogramas} son extrapolables conforme se incrementa la dimensionalidad de los datos, en el caso de los esquemas de estimación, esto sucede de manera directa. Sin embargo, en el caso de los esquemas de particionamiento surgen distintos problemas debido al crecimiento exponencial tanto del espacio como del tiempo conforme aumenta el número de dimensiones de los datos.

        \paragraph{}
        Los \emph{Histogramas} representan una estrategia sencilla, tanto a nivel de construcción como de consulta, la cual ofrece buenos resultados en un gran número de casos. Dichas estructuras han sido ampliamente probadas para aproximación de consultas relacionadas con suma de rangos o frecuencias puntuales. Tal y como se ha dicho previamente, su comportamiento en el caso unidimensional ha sido ampliamente estudiado, sin embargo, debido al crecimiento exponencial a nivel de complejidad conforme las dimensiones del conjunto de datos aumentan, estas estrategias son descartadas en muchas ocasiones. Los \emph{Histogramas} requieren de un conjunto de parámetros fijados \emph{a-priori}, los cuales afectan en gran medida al grado de precisión de los mismos (pero cuando se seleccionan de manera adecuada esta solución goza de una gran cercanía al punto de optimalidad), por tanto, en casos en que la estimación de dichos valores necesarios \emph{a-priori} se convierte en una labor complicada existen otras técnicas que ofrecen mejores resultados.

      \subsection{Wavelet}
      \label{sec:wavelet}

        \paragraph{}
        [TODO ]

        \subsubsection{Haar Wavelet Transform}

          \paragraph{}
          [TODO ]

        \paragraph{}
        [TODO hablar sobre tipos de error]

        \begin{itemize}

          \item \textbf{Wavelets Restringidas}: [TODO ]

          \item \textbf{Wavelets No Restringidas}: [TODO ]

        \end{itemize}

        \paragraph{}
        [TODO hablar de problemas de dimensionalidad]

        \paragraph{}
        [Todo hablar de ventajas e inconvenientes]

      \subsection{Sketch}
      \label{sec:sketch}

        \paragraph{}
        [TODO ]

    \section{Count-Min Sketch}
    \label{sec:count_min_sketch}

      \paragraph{}
      [TODO ] \emph{An improved data stream summary: the count-min sketch and its applications} \cite{cormode2005improved}

    \section{Count Sketch}
    \label{sec:count_sketch}

      \paragraph{}
      [TODO ] \emph{Finding frequent items in data streams} \cite{charikar2002finding}

    \section{AMS Sketch}
    \label{sec:ams_sketch}

      \paragraph{}
      [TODO ] \emph{The space complexity of approximating the frequency moments} \cite{alon1996space}

    \section{HyperLogLog}
    \label{sec:hyper_log_log}

      \paragraph{}
      [TODO ] \emph{Hyperloglog: the analysis of a near-optimal cardinality estimation algorithm} \cite{flajolet2007hyperloglog}


\end{document}
